# API Parameter Coverage & Test Scenario Generator

A comprehensive Python tool for generating test scenarios from OpenAPI/Swagger schemas using LLM-powered analysis and Business Requirement Document (BRD) integration. The tool automatically analyzes API schemas, cross-references them with business requirements, and generates comprehensive Gherkin test scenarios with detailed analytics.

## ğŸš€ Features

### Core Capabilities
- ğŸ”„ **Multi-Format Schema Support**: Handles Swagger 2.0, OpenAPI 3.0, and OpenAPI 3.1 (JSON/YAML)
- ğŸ“¥ **Automatic Schema Download**: Fetches schemas from URLs with validation
- ğŸ” **Deep Schema Analysis**: Extracts parameters, constraints, and complexity metrics
- ğŸ“‹ **BRD Integration**: Business Requirement Document support for scope-based testing
- ğŸ¤– **LLM-Powered Generation**: Uses OpenAI GPT-4 for intelligent test scenario generation
- ğŸ¯ **Smart Scope Filtering**: Cross-references BRD with Swagger to test only required endpoints
- ğŸ“Š **CSV Export**: Export test scenarios to CSV format
- ğŸ“ˆ **Comprehensive Analytics**: Detailed metrics and reports for every algorithm execution

### Advanced Features
- ğŸ“¦ **Smart Chunking**: Automatically handles large schemas by processing in chunks
- ğŸ”„ **BRD Generation**: Creates BRD documents from Swagger schemas using heuristic analysis
- ğŸ“„ **Document Parsing**: Converts BRD documents (PDF, Word, TXT, CSV) to structured schemas
- ğŸ“Š **Algorithm Tracking**: Detailed complexity analysis for each algorithm execution
- âš¡ **Performance Monitoring**: Execution time and resource usage tracking
- ğŸ¨ **Structured Reports**: Separate analytics reports for each algorithm and LLM call
- ğŸ“ˆ **Coverage Analysis**: Analyzes test coverage against BRD requirements
- ğŸ“Š **Analytics Dashboard**: Aggregates analytics across runs with trend analysis
- âš™ï¸ **Configuration Management**: YAML/JSON config files with environment-specific settings

## ğŸ“‹ Table of Contents

- [Installation](#installation)
- [Quick Start](#quick-start)
- [Usage Guide](#usage-guide)
- [Project Structure](#project-structure)
- [Modules Overview](#modules-overview)
- [BRD System](#brd-system)
- [Analytics & Reporting](#analytics--reporting)
- [Configuration](#configuration)
- [Testing](#testing)
- [Troubleshooting](#troubleshooting)

## ğŸ’» Installation

### Prerequisites

- Python 3.8 or higher
- OpenAI API key (for LLM features)
- Internet connection (for schema downloading)

### Step-by-Step Setup

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd api-param-coverage
   ```

2. **Create a virtual environment**
   ```bash
   python3 -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Set up environment variables**
   
   Create a `.env` file in the project root:
   ```bash
   OPENAI_API_KEY=your-api-key-here
   ```
   
   **âš ï¸ Important**: Never commit your API key. The `.env` file is already in `.gitignore`.

5. **Optional: Install document parsing dependencies**
   
   For BRD document parsing (PDF, Word):
   ```bash
   pip install PyPDF2 python-docx
   ```

## ğŸš€ Quick Start

```bash
python main.py
```

The tool will guide you through:
1. Entering a Swagger/OpenAPI schema URL
2. Processing and analyzing the schema
3. Handling BRD (load existing, generate new, or parse from document)
4. Generating test scenarios
5. Exporting to CSV format

## ğŸ“– Usage Guide

### Basic Workflow

1. **Schema Input**: Provide a Swagger/OpenAPI schema URL
2. **Schema Processing**: The tool downloads, validates, and processes the schema
3. **BRD Handling**: Choose one of three options:
   - **Load Existing BRD**: Select from saved BRD schema files
   - **Generate BRD**: Create a new BRD from the Swagger schema using LLM
   - **Parse BRD Document**: Convert a BRD document (PDF, Word, TXT, CSV) to schema format
4. **Scope Filtering**: Cross-reference BRD with Swagger to filter endpoints
5. **Test Generation**: Generate Gherkin scenarios only for BRD-covered endpoints
6. **Export**: Save results to CSV format
7. **Analytics**: Review detailed analytics and algorithm reports

### Example Session

```bash
$ python main.py
======================================================================
Swagger Schema Processor & Test Scenario Generator
======================================================================

Enter Swagger/OpenAPI schema URL: https://petstore.swagger.io/v2/swagger.json

======================================================================
Step 1: Downloading schema...
======================================================================
âœ“ Schema downloaded: data/schemas/petstore_swagger_io_v2_swagger.json

======================================================================
Step 2: Processing schema...
======================================================================
âœ“ Schema processed:
  - API: Swagger Petstore
  - Endpoints: 20

======================================================================
Step 3: Analyzing schema for test traceability...
======================================================================
âœ“ Schema analyzed:
  - Endpoints analyzed: 20

======================================================================
Step 4: Business Requirement Document (BRD)...
======================================================================

Do you have a BRD schema file, or would you like to generate one from the Swagger schema?
1. Load existing BRD file
2. Generate BRD from Swagger schema (using LLM)

Enter choice (1 or 2): 2

ğŸ“‹ Generating BRD from Swagger schema...
âœ“ BRD generated: API Test Requirements Document
  - Requirements: 15
  - Saved to: reference/brd/input/petstore_swagger_io_v2_swagger_brd.json

======================================================================
Step 5: Cross-referencing BRD with Swagger schema...
======================================================================
âœ“ Cross-reference complete:
  - Total endpoints: 20
  - BRD covered: 15
  - Not covered: 5
  - Coverage: 75.0%
ğŸ“ˆ Cross-reference report saved: output/analytics/reports/20241124_133002_cross_reference_schemacrossreference.txt

======================================================================
Step 6: Generating Gherkin test scenarios via LLM...
======================================================================
ğŸ¤– Sending prompt to gpt-4...
âœ“ Gherkin scenarios generated
ğŸ“Š Analytics saved: output/20241124_133045_schema/analytics/20241124_133045.txt
ğŸ“ˆ Algorithm report saved: output/20241124_133045_schema/analytics/reports/llm_prompter_*.txt

======================================================================
Step 7: Saving to CSV...
======================================================================
âœ“ CSV saved: output/20241124_133045_petstore/petstore_swagger_io_v2_swagger-20241124_133045.csv

======================================================================
Summary
======================================================================
Schema: petstore_swagger_io_v2_swagger.json
API: Swagger Petstore
Total Endpoints: 20
BRD: API Test Requirements Document
BRD Coverage: 75.0%
Tested Endpoints: 15
Output: output/20241124_133045_petstore/petstore_swagger_io_v2_swagger-20241124_133045.csv

âœ“ Processing complete!
```

## ğŸ“ Project Structure

```
api-param-coverage/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ modules/
â”‚       â”œâ”€â”€ swagger/                   # Schema downloading and validation
â”‚       â”‚   â”œâ”€â”€ schema_fetcher.py      # Downloads schemas from URLs
â”‚       â”‚   â””â”€â”€ schema_validator.py    # Validates and normalizes schemas
â”‚       â”œâ”€â”€ engine/                    # Core processing engine
â”‚       â”‚   â”œâ”€â”€ algorithms/            # Schema processing algorithms
â”‚       â”‚   â”‚   â”œâ”€â”€ processor.py       # Schema processing
â”‚       â”‚   â”‚   â”œâ”€â”€ analyzer.py        # Schema analysis and complexity
â”‚       â”‚   â”‚   â”œâ”€â”€ csv_generator.py   # CSV export
â”‚       â”‚   â”œâ”€â”€ analytics/             # Analytics and reporting
â”‚       â”‚   â”‚   â”œâ”€â”€ metrics_collector.py      # Metrics collection
â”‚       â”‚   â”‚   â”œâ”€â”€ algorithm_tracker.py      # Algorithm tracking
â”‚       â”‚   â”‚   â”œâ”€â”€ aggregator.py     # Analytics aggregation
â”‚       â”‚   â”‚   â””â”€â”€ dashboard.py      # Analytics dashboard
â”‚       â”‚   â”œâ”€â”€ coverage/              # Test coverage analysis
â”‚       â”‚   â”‚   â””â”€â”€ coverage_analyzer.py  # Coverage analysis
â”‚       â”‚   â””â”€â”€ llm/                   # LLM integration
â”‚       â”‚       â””â”€â”€ prompter.py        # LLM prompting and generation
â”‚       â”œâ”€â”€ brd/                       # Business Requirement Document
â”‚       â”‚   â”œâ”€â”€ brd_schema.py          # BRD schema definitions
â”‚       â”‚   â”œâ”€â”€ brd_loader.py          # BRD file I/O
â”‚       â”‚   â”œâ”€â”€ brd_parser.py          # Document parsing (PDF, Word, etc.)
â”‚       â”‚   â”œâ”€â”€ brd_validator.py       # BRD validation
â”‚       â”‚   â””â”€â”€ schema_cross_reference.py  # BRD-Swagger cross-reference
â”‚       â”œâ”€â”€ brd_generator/             # BRD generation
â”‚       â”‚   â””â”€â”€ brd_generator.py      # LLM-based BRD generator
â”œâ”€â”€ tests/                             # Test suite
â”‚   â”œâ”€â”€ test_schema_fetcher.py
â”‚   â”œâ”€â”€ test_schema_validator.py
â”‚   â”œâ”€â”€ test_processor.py
â”‚   â”œâ”€â”€ test_analyzer.py
â”‚   â”œâ”€â”€ test_llm_prompter.py
â”‚   â”œâ”€â”€ test_csv_generator.py
â”‚   â”œâ”€â”€ test_brd_schema.py
â”‚   â”œâ”€â”€ test_brd_loader.py
â”‚   â””â”€â”€ test_schema_cross_reference.py
â”œâ”€â”€ reference/                         # Reference data and templates
â”‚   â”œâ”€â”€ schemas/                      # Downloaded schemas
â”‚   â”œâ”€â”€ brd/
â”‚   â”‚   â”œâ”€â”€ input/                     # BRD documents (PDF, Word, TXT, CSV)
â”‚   â”‚   â”‚   â””â”€â”€ README.md              # BRD document input guide
â”‚   â”‚   â””â”€â”€ output/                    # Generated BRD schemas (JSON)
â”‚   â”‚       â””â”€â”€ README.md              # BRD schema format documentation
â”‚   â””â”€â”€ dummy_data/                   # Example data and scripts
â”‚       â””â”€â”€ scripts/                   # Example utility scripts
â”œâ”€â”€ docs/                              # Documentation
â”‚   â”œâ”€â”€ PROJECT_STATUS.md              # Project status
â”‚   â”œâ”€â”€ NEXT_STEPS.md                  # Roadmap
â”‚   â””â”€â”€ README.md                      # Documentation guide
â”œâ”€â”€ output/                            # Execution outputs (at project root)
â”‚   â”œâ”€â”€ <timestamp>-<filename>/       # Execution run folders
â”‚   â”‚   â”œâ”€â”€ scenarios/                 # CSV scenarios subfolder
â”‚   â”‚   â”‚   â””â”€â”€ <timestamp>_*_scenarios.csv
â”‚   â”‚   â”œâ”€â”€ analytics/                 # Analytics subfolder
â”‚   â”‚   â”‚   â””â”€â”€ <timestamp>_*.txt      # LLM execution metrics
â”‚   â”‚   â”œâ”€â”€ validation/                # Validation reports subfolder
â”‚   â”‚   â”‚   â””â”€â”€ <timestamp>_brd_validation_report.txt
â”‚   â”‚   â””â”€â”€ reports/                   # Algorithm reports subfolder
â”‚   â”‚       â””â”€â”€ <timestamp>_*_algorithm_*.txt
â”‚   â””â”€â”€ example_weather_api/           # Example output (weather.gov API)
â”‚       â”œâ”€â”€ scenarios/                  # Example CSV scenarios
â”‚       â”œâ”€â”€ analytics/                  # Example analytics
â”‚       â”œâ”€â”€ validation/                  # Example validation reports
â”‚       â”œâ”€â”€ reports/                     # Example algorithm reports
â”‚       â””â”€â”€ README.md                   # Example output documentation
â”œâ”€â”€ main.py                            # Main entry point
â”œâ”€â”€ reference/                         # Reference data and templates
â”‚   â”œâ”€â”€ schemas/                      # Downloaded schemas
â”‚   â”œâ”€â”€ brd/
â”‚   â”‚   â”œâ”€â”€ input/                     # BRD documents (PDF, Word, TXT, CSV)
â”‚   â”‚   â”‚   â””â”€â”€ README.md              # BRD document input guide
â”‚   â”‚   â””â”€â”€ output/                    # Generated BRD schemas (JSON)
â”‚   â”‚       â””â”€â”€ README.md              # BRD schema format documentation
â”‚   â””â”€â”€ scripts/                       # Example utility scripts
â”‚       â”œâ”€â”€ README.md                  # Scripts documentation
â”‚       â””â”€â”€ run_weather_api.py         # Weather API example script
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ pytest.ini                         # Pytest configuration
â””â”€â”€ README.md                          # This file
```

## ğŸ”§ Modules Overview

### Schema Processing

| Module | File | Description |
|--------|------|-------------|
| Schema Fetcher | `swagger/schema_fetcher.py` | Downloads schemas from URLs, supports JSON/YAML formats, automatic format detection, error handling and retry logic |
| Schema Validator | `swagger/schema_validator.py` | Detects schema type (Swagger 2.0, OpenAPI 3.0, OpenAPI 3.1), validates schema structure, normalizes schemas, handles partial schemas |
| Schema Processor | `engine/algorithms/processor.py` | Extracts API metadata, processes endpoints and HTTP methods, extracts components (schemas, parameters, responses) |
| Schema Analyzer | `engine/algorithms/analyzer.py` | Deep analysis of schema structure, extracts all parameters, computes iteration domains, handles nested structures and `$ref` references, calculates complexity metrics |

### LLM Integration

| Module | File | Description |
|--------|------|-------------|
| LLM Prompter | `engine/llm/prompter.py` | Creates optimized prompts from schema analysis, integrates with multiple LLM providers, generates Gherkin test scenarios, automatic chunking, token limit management, retry logic |

### BRD System

| Module | File | Description |
|--------|------|-------------|
| BRD Schema | `brd/brd_schema.py` | Defines structured BRD schema format, requirement and test scenario models, priority and status enums, type-safe data structures |
| BRD Loader | `brd/brd_loader.py` | Loads BRD schemas from JSON files, saves BRD schemas, lists available BRD files, validates BRD structure |
| BRD Parser | `brd/brd_parser.py` | Parses BRD documents from multiple formats (PDF, Word, TXT, CSV, Markdown), uses LLM to extract structured data, converts to BRD schema format |
| BRD Generator | `brd_generator/brd_generator.py` | Generates BRD from Swagger schemas, uses heuristic analysis, LLM-powered requirement generation, priority determination, test scenario suggestions |
| Schema Cross-Reference | `brd/schema_cross_reference.py` | Cross-references BRD requirements with Swagger endpoints, filters endpoints by BRD coverage, generates coverage reports, calculates coverage percentages |

### Analytics & Reporting

| Module | File | Description |
|--------|------|-------------|
| Metrics Collector | `engine/analytics/metrics_collector.py` | Collects metrics for LLM API calls, tracks execution time and token usage, analyzes complexity metrics, generates formatted reports |
| Algorithm Tracker | `engine/analytics/algorithm_tracker.py` | Tracks algorithm execution, analyzes input/output complexity, calculates algorithm-specific metrics, generates detailed algorithm reports |
| Analytics Aggregator | `engine/analytics/aggregator.py` | Aggregates analytics data across multiple execution runs, generates summary reports, tracks trends over time |
| Analytics Dashboard | `engine/analytics/dashboard.py` | Generates comprehensive dashboard reports, cost analysis for LLM calls, trend analysis and recommendations, text-based visualization reports |

### Export

| Module | File | Description |
|--------|------|-------------|
| CSV Generator | `engine/algorithms/csv_generator.py` | Parses Gherkin content, converts to structured CSV format, handles markdown code blocks, extracts features, scenarios, and steps |

### Test Coverage Analysis

| Module | File | Description |
|--------|------|-------------|
| Coverage Analyzer | `engine/coverage/coverage_analyzer.py` | Compares generated Gherkin scenarios with BRD requirements, calculates coverage percentage per requirement, identifies missing test scenarios, generates detailed coverage reports, identifies coverage gaps prioritized by requirement priority |

### Configuration Management

| Module | File | Description |
|--------|------|-------------|

### Interactive CLI

| Module | File | Description |
|--------|------|-------------|
| CLI Utilities | `cli/cli_utils.py` | Progress Bars (visual progress indicators), Status Updates (real-time status messages), Interactive Selection (validated interactive selection with retry support), Error Recovery (error handling with recovery options), User Confirmation (confirmation prompts with defaults), Formatted Output (consistent formatting for sections, success, errors, warnings, and info messages) |

## ğŸ“‹ BRD System

### What is a BRD?

A Business Requirement Document (BRD) defines which API endpoints and scenarios should be tested based on business requirements. The tool uses BRD to filter and focus test generation on only the endpoints that matter.

### BRD Schema Format

BRD files are stored in JSON format in `reference/brd/input/`. See `reference/brd/input/README.md` for the complete schema format.

**Key Components:**
- **Requirements**: List of business requirements
- **Endpoints**: API endpoint paths and methods
- **Test Scenarios**: Specific test cases for each requirement
- **Priority**: Requirement priority (critical, high, medium, low)
- **Acceptance Criteria**: Success criteria for requirements

### BRD Workflow

1. **Create BRD**: Generate from Swagger or parse from document
2. **Validate**: System validates BRD against Swagger schema
3. **Cross-Reference**: Match BRD requirements with Swagger endpoints
4. **Filter**: Only test BRD-covered endpoints
5. **Generate**: Create test scenarios for filtered scope

## ğŸ“Š Analytics & Reporting

### Analytics Files

All analytics are saved in `docs/<timestamp>_<schema>/`:

- **LLM Execution Metrics** (`YYYYMMDD_HHMMSS.txt`): General LLM call metrics
- **Algorithm Reports** (`reports/YYYYMMDD_HHMMSS_<type>_<name>.txt`): Detailed algorithm analysis

### Metrics Tracked

#### LLM Metrics
- Execution time
- Token usage (prompt, completion, total)
- Prompt and response sizes
- Model information
- Task type

#### Algorithm Metrics
- Algorithm name and type
- Input complexity (size, structure, depth)
- Output complexity (quality, element count)
- Execution time
- Algorithm-specific complexity metrics

#### Complexity Analysis
- Total endpoints analyzed
- Parameter counts and distributions
- Constraint analysis (enum, pattern, bounded/unbounded)
- Iteration domain counts
- Coverage percentages

### Report Structure

Each algorithm report includes:
1. **Algorithm Information**: Name, type, execution time
2. **Input Analysis**: Complexity metrics for input data
3. **Output Analysis**: Quality and complexity of output
4. **Algorithm-Specific Metrics**: Custom metrics per algorithm
5. **LLM Analysis** (if applicable): Token usage and prompt metrics

## âš™ï¸ Configuration

### Configuration Files

The tool supports YAML and JSON configuration files for flexible settings management:

**Configuration File Priority:**
1. Environment-specific config: `config/{environment}.yaml` (e.g., `config/production.yaml`)
2. Main config file: `config.yaml` or `config.json`
3. Environment variables (highest priority, overrides files)
4. Default values (if nothing is configured)

**Example Configuration (`config.yaml`):**
```yaml
environment: development

algorithm:
  chunk_size: 12
  chunking_threshold: 15
  max_tokens: 3000
  retry_attempts: 3

paths:
  schemas_dir: reference/schemas
  output_dir: output
  analytics_dir: output/analytics

llm:
  model: gpt-4
  temperature: 0.7
  max_tokens: 3000

debug: false
verbose: false
```

**Environment-Specific Configuration:**
- Set `APP_ENV` environment variable to use environment-specific configs
- Example: `APP_ENV=production` loads `config/production.yaml`
- See `config/development.yaml.example` and `config/production.yaml.example` for examples

### Environment Variables

| Variable | Description | Required | Overrides Config |
|----------|-------------|----------|------------------|
| `OPENAI_API_KEY` | OpenAI API key for LLM features | Yes | Yes |
| `APP_ENV` | Environment name (development, production, testing) | No | Yes |
| `LLM_MODEL` | LLM model to use | No | Yes |
| `LLM_MAX_TOKENS` | Maximum response tokens | No | Yes |
| `LLM_TEMPERATURE` | LLM temperature setting | No | Yes |
| `CHUNK_SIZE` | Endpoints per chunk | No | Yes |
| `CHUNKING_THRESHOLD` | Endpoints before chunking | No | Yes |
| `OUTPUT_DIR` | Output directory path | No | Yes |
| `SCHEMAS_DIR` | Schema storage directory | No | Yes |
| `DEBUG` | Enable debug mode | No | Yes |
| `VERBOSE` | Enable verbose output | No | Yes |

### Default Settings

| Setting | Default Value | Description |
|---------|---------------|-------------|
| Schema storage | `reference/schemas/` | Downloaded schemas location |
| CSV output directory | `output/<timestamp>-<filename>/` | Generated CSV files location |
| Analytics directory | `output/<timestamp>-<filename>/analytics/` | Analytics files location |
| BRD directory | `reference/brd/output/` | BRD schema files location |
| LLM model | `gpt-4` | OpenAI model to use |
| Max tokens | `3000` | Maximum response tokens |
| Temperature | `0.7` | LLM temperature setting |

### Supported Schema Types

- **Swagger 2.0** (JSON/YAML)
- **OpenAPI 3.0.0** (JSON/YAML)
- **OpenAPI 3.1.0** (JSON/YAML)
- Partial schemas (with warnings)
- Schemas with missing optional fields (normalized)

## ğŸ§ª Testing

### Running Tests

```bash
# Run all tests
pytest

# Run with verbose output
pytest -v

# Run with coverage report
pytest --cov=src --cov-report=html

# Run specific test file
pytest tests/test_analyzer.py

# Run specific test class
pytest tests/test_llm_prompter.py::TestLLMPrompter
```

### Test Coverage

The project includes comprehensive tests for:

- âœ… Schema fetching and validation
- âœ… Schema processing and analysis
- âœ… LLM prompting and validation
- âœ… CSV generation
- âœ… CSV export
- âœ… BRD schema operations
- âœ… BRD parsing and generation
- âœ… Schema cross-referencing
- âœ… Analytics and metrics collection

### Test Files

| File | Feature |
|------|---------|
| `test_schema_fetcher.py` | Schema downloading tests |
| `test_schema_validator.py` | Schema validation tests |
| `test_processor.py` | Schema processing tests |
| `test_analyzer.py` | Schema analysis tests |
| `test_llm_prompter.py` | LLM integration tests |
| `test_csv_generator.py` | CSV generation tests |
| `test_brd_*.py` | BRD module tests |
| `test_coverage_analyzer.py` | Coverage analysis tests |

## ğŸ“¤ Output Format

### CSV Files

CSV files are saved in `output/<timestamp>-<filename>/` with format: `<filename>-<timestamp>.csv`

**Columns:**
- `Feature`: Gherkin feature name
- `Scenario`: Scenario name
- `Tags`: Scenario tags (comma-separated)
- `Given`: Given steps (semicolon-separated)
- `When`: When steps (semicolon-separated)
- `Then`: Then steps (semicolon-separated)
- `All Steps`: All steps combined

### Analytics Reports

**LLM Execution Metrics** (`output/<timestamp>-<filename>/analytics/*.txt`):
- Execution information
- API information
- Schema statistics
- Complexity analysis
- Prompt metrics
- API usage (actual tokens)
- Response metrics

**Algorithm Reports** (`output/<timestamp>-<filename>/analytics/reports/*_algorithm_*.txt`):
- Algorithm information
- Input complexity analysis
- Output complexity analysis
- Algorithm-specific metrics
- LLM call analysis (if applicable)

## ğŸ” Troubleshooting

### Common Issues

#### Empty CSV Files

**Symptoms**: CSV files contain only placeholders or are empty

**Solutions**:
1. Verify `OPENAI_API_KEY` is set correctly in `.env`
2. Check network connectivity to OpenAI API
3. Verify schema has analyzable endpoints
4. Review console output for error messages

#### Schema Validation Errors

**Symptoms**: Warnings about missing fields or invalid structure

**Solutions**:
- Missing optional fields are normalized automatically
- Partial schemas may generate warnings but still work
- Check error messages for specific issues
- Verify schema matches OpenAPI/Swagger specification

#### LLM Generation Failures

**Common Causes**:
- **Rate Limits**: Wait and retry (automatic retry included)
- **Invalid API Key**: Check `.env` file
- **Insufficient Quota**: Check OpenAI account billing
- **Empty Endpoints**: Ensure schema has endpoints
- **Token Limits**: Large schemas are automatically chunked

**Solutions**:
- Check API key validity
- Verify OpenAI account has credits
- Review token usage in analytics reports
- Consider using smaller schema subsets

#### BRD Parsing Issues

**Symptoms**: BRD parsing fails or produces incomplete results

**Solutions**:
- Ensure document format is supported
- Install required dependencies (PyPDF2, python-docx)
- Check document structure and formatting
- Review LLM parsing logs in analytics

#### Token Limit Errors

**Symptoms**: "context_length_exceeded" errors

**Solutions**:
- Tool automatically chunks large schemas
- If errors persist, schema may be extremely large
- Consider using `gpt-4-turbo` with larger context window
- Review chunk size settings

## ğŸ“š Additional Resources

### Documentation

- **BRD Schema Format**: `reference/brd/input/README.md`
- **Project Status**: `docs/PROJECT_STATUS.md`
- **Next Steps**: `docs/NEXT_STEPS.md`

### Dependencies

See `requirements.txt` for complete dependency list.

**Core Dependencies:**
- `requests`: HTTP requests for schema downloading
- `pyyaml`: YAML parsing
- `openai`: OpenAI API client
- `python-dotenv`: Environment variable management

**Optional Dependencies:**
- `PyPDF2`: PDF document parsing
- `python-docx`: Word document parsing

## ğŸ“„ License

This project is provided as-is for testing and development purposes.
